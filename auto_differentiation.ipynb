{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## understand automatic differentiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分求解分为4类：\n",
    "1. 手动求解 （manual differentiation） ：\n",
    "    计算梯度公式，代入实际数值；\n",
    "2. 数值微分（Numerical differentiation）：\n",
    "    导数定义 (f(x+d)-f(x))/d  d->0, 但是比较慢(计算两次函数值)，且有截断误差roundoff （比如sin（x）泰勒展开，只计算前几项，后面截断）和舍入误差truncation error（计算不精确）； 一般可以用来验证梯度实现的正确性；\n",
    "3. 符号微分（symbolic differentiation）：\n",
    "    对数学表达式自动微分求解，但是会有表达式膨胀的问题，比如 16a(1-a)((1-2a)^2) 对a 求偏导: 16(1-a)((1-2a)^2)-16a((1-2a)^2)-64a(1-a)(1-2a)\n",
    "4. 自动微分 （Automatic differentiation）：\n",
    "    自动微分法是一种介于符号微分和数值微分的方法：数值微分强调一开始直接代入数值近似求解；符号微分强调直接对代数进行求解，最后才代入问题数值；自动微分将符号微分法应用于最基本的算子，比如常数，幂函数，指数函数，对数函数，三角函数等，然后代入数值，保留中间结果，最后再应用于整个函数。因此它应用相当灵活，可以做到完全向用户隐藏微分求解过程，由于它只对基本函数或常数运用符号微分法则，所以它可以灵活结合编程语言的循环结构，条件结构等，使用自动微分和不使用自动微分对代码总体改动非常小，并且由于它的计算实际是一种图计算，可以对其做很多优化. 分为forwoard mode 和reverse mode。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向模式在计算图前向传播时同时计算微分\n",
    "$ x -> x + epsilon$\n",
    "在计算时： $x*x -> (x+epsilon)*(x+epsilon) = x*x + 2*x*epsilon + epsilon*epsilon$\n",
    "\n",
    "假设 $epsilon*epsilon=0$则：\n",
    "$x*x -> (x+epsilon)*(x+epsilon) = x*x + 2*x*epsilon$\n",
    "梯度为 $\\frac{x*x + 2*x*epsilon - x*x}{epsilon}=2x$, 梯度为epsilon的系数\n",
    "\n",
    "\n",
    "则前向的结果是： $x*x$; 后向梯度是$2*x$\n",
    "\n",
    "\n",
    "假设是数值法：\n",
    "$\\frac{d(x*x)}{dx}=\\frac{(x+epsilon)*(x+epsilone)-x*x}{epsilon} = \\frac{2*epsilon*x}{epsilon}=2x$\n",
    "\n",
    "则把每一个变量增加一个域去计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n",
      "[8, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,val=0):\n",
    "        self.val=val\n",
    "        self.grads=[1]\n",
    "\n",
    "class Op(object):\n",
    "    def __init__(self):\n",
    "        self.node=Node()\n",
    "    def compute(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "\n",
    "        \n",
    "# 实际计算在各个操作算子上        \n",
    "class MulOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        self.node.val=node1.val*node2.val\n",
    "        self.node.grads=[0]*len(node1.grads+node2.grads)\n",
    "        for i in range(len(node1.grads)):\n",
    "            self.node.grads[i]+=node2.val*node1.grads[i]\n",
    "        for i in range(len(node2.grads)):\n",
    "            self.node.grads[len(node1.grads)+i]+=node1.val*node2.grads[i]  \n",
    "        \n",
    "'''\n",
    "a = 1 + e1\n",
    "b = 2 + e2\n",
    "a*b = (2 + 2e1 + 1e2) \n",
    "\n",
    "grad(a) = 2\n",
    "grad(b) = 1\n",
    "'''\n",
    "\n",
    "# x1*x2*x3 对x1, x2, x3 分别求偏导数\n",
    "node1=Node(1)\n",
    "node2=Node(2)\n",
    "node3=Node(4)\n",
    "\n",
    "mul_op=MulOp()\n",
    "mul_op.compute(node1,node2)\n",
    "\n",
    "mul_op1=MulOp()\n",
    "mul_op1.compute(mul_op.node,node3)\n",
    "\n",
    "print(mul_op.node.grads)\n",
    "print(mul_op1.node.grads)\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward result: 11.652071455223084\n",
      "the differential of final result to node1:  5.5\n",
      "the differential of final result to node2:  1.7163378145367738\n",
      "8\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,inputs,grads=1):\n",
    "        self.val=inputs\n",
    "        self.grad=grads\n",
    "\n",
    "class Op(object):\n",
    "    def __init__(self,name):\n",
    "        self._name=name\n",
    "        self.node=Node(name)\n",
    "        self.inputs=[]\n",
    "        self.node.grads=0\n",
    "    def compute(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "    def gradient(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "        \n",
    "\n",
    "# 实际计算在各个操作算子上   \n",
    "class Const(Op):\n",
    "    def compute(self, val):\n",
    "        self.node.val=val\n",
    "        self.inputs=[]\n",
    "        return val\n",
    "    def gradient(self,out_grad):\n",
    "        grads=out_grad\n",
    "        self.node.grads=grads\n",
    "        return grads\n",
    "\n",
    "       \n",
    "class MulOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs= node1.val * node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "    \n",
    "    def gradient(self, out_grad):\n",
    "        self.node.grads=out_grad\n",
    "        grads=[self.inputs[1].val * out_grad ,self.inputs[0].val*out_grad]\n",
    "        self.inputs[0].grads+=grads[0]\n",
    "        self.inputs[1].grads+=grads[1]\n",
    "        \n",
    "class AddOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs = node1.val+ node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "        \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad\n",
    "        self.inputs[1].grads+=out_grad\n",
    "        \n",
    "        \n",
    "class SubOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs = node1.val-node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "        \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad\n",
    "        self.inputs[1].grads-=out_grad\n",
    "\n",
    "class In(Op):\n",
    "    def compute(self, node1):\n",
    "        inputs=math.log(node1.val,math.e)\n",
    "        self.inputs=[node1]\n",
    "        self.node.val=inputs\n",
    "    \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad*(1/self.inputs[0].val)\n",
    "    \n",
    "class Sin(Op):\n",
    "    def compute(self, node):\n",
    "        inputs=np.sin(node.val)\n",
    "        self.inputs=[node]\n",
    "        self.node.val=inputs\n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad*np.cos(self.inputs[0].val)\n",
    "    \n",
    "\n",
    "# ln(x)+x*y+sin(y) 对x 和 y 求导\n",
    "node1=Const(\"const1\")\n",
    "node1.compute(2)\n",
    "node2=Const(\"const2\")\n",
    "node2.compute(5)\n",
    "\n",
    "in_op=In(\"in\")\n",
    "in_op.compute(node1.node)\n",
    "mul_op=MulOp(\"mul1\")\n",
    "mul_op.compute(node1.node,node2.node)\n",
    "sin_op=Sin(\"sin\")\n",
    "sin_op.compute(node2.node)\n",
    "add_op=AddOp(\"add\")\n",
    "add_op.compute(in_op.node,mul_op.node)\n",
    "sub_op=SubOp(\"sub\")\n",
    "sub_op.compute(add_op.node,sin_op.node)\n",
    "print(\"forward result:\",sub_op.node.val)\n",
    "\n",
    "sub_op.gradient(1)\n",
    "add_op.gradient(sub_op.inputs[0].grads)\n",
    "in_op.gradient(add_op.inputs[0].grads)\n",
    "mul_op.gradient(add_op.inputs[1].grads)\n",
    "sin_op.gradient(sub_op.inputs[1].grads)\n",
    "print(\"the differential of final result to node1: \", node1.node.grads)\n",
    "print(\"the differential of final result to node2: \", node2.node.grads)\n",
    "\n",
    "\n",
    "node1=Const(\"const1\")\n",
    "node1.compute(1)\n",
    "node2=Const(\"const2\")\n",
    "node2.compute(2)\n",
    "node3=Const(\"const3\")\n",
    "node3.compute(4)\n",
    "\n",
    "# x1*x2*x3 对x1, x2, x3 分别求偏导数\n",
    "mul_op=MulOp(\"mul1\")\n",
    "mul_op1=MulOp(\"mul2\")\n",
    "mul_op.compute(node1.node,node2.node)\n",
    "mul_op1.compute(mul_op.node,node3.node)\n",
    "mul_op1.gradient(1)\n",
    "mul_op.gradient(mul_op1.inputs[0].grads)\n",
    "print(node1.node.grads)\n",
    "print(node2.node.grads)\n",
    "print(node3.node.grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
