{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## understand automatic differentiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微分求解分为4类：\n",
    "1. 手动求解 （manual differentiation） ：\n",
    "    计算梯度公式，代入实际数值；\n",
    "2. 数值微分（Numerical differentiation）：\n",
    "    导数定义 (f(x+d)-f(x))/d  d->0, 但是比较慢(计算两次函数值)，且有截断误差roundoff （比如sin（x）泰勒展开，只计算前几项，后面截断）和舍入误差truncation error（计算不精确）； 一般可以用来验证梯度实现的正确性；\n",
    "3. 符号微分（symbolic differentiation）：\n",
    "    对数学表达式自动微分求解，但是会有表达式膨胀的问题，比如 16a(1-a)((1-2a)^2) 对a 求偏导: 16(1-a)((1-2a)^2)-16a((1-2a)^2)-64a(1-a)(1-2a)\n",
    "4. 自动微分 （Automatic differentiation）：\n",
    "    自动微分法是一种介于符号微分和数值微分的方法：数值微分强调一开始直接代入数值近似求解；符号微分强调直接对代数进行求解，最后才代入问题数值；自动微分将符号微分法应用于最基本的算子，比如常数，幂函数，指数函数，对数函数，三角函数等，然后代入数值，保留中间结果，最后再应用于整个函数。因此它应用相当灵活，可以做到完全向用户隐藏微分求解过程，由于它只对基本函数或常数运用符号微分法则，所以它可以灵活结合编程语言的循环结构，条件结构等，使用自动微分和不使用自动微分对代码总体改动非常小，并且由于它的计算实际是一种图计算，可以对其做很多优化. 分为forwoard mode 和reverse mode。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向模式在计算图前向传播时同时计算微分\n",
    "$ v_i=\\frac{delta(v_i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "class Op(object):\n",
    "    def compute(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "    def gradient(self, nodes)\n",
    "        assert False, \"Implemented in subclass\"\n",
    "        \n",
    "# 实际计算在各个操作算子上        \n",
    "class MulOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        return node1.real * node2.real\n",
    "    def gradient(self, node1, node2):\n",
    "        return node1.real*node2.grad\n",
    "        \n",
    "        \n",
    "        \n",
    "'''\n",
    "a = 1 + i\n",
    "b = 2 + j\n",
    "a*b = (2 + 2i + 1j + 2ij) \n",
    "ij=0 // not real value\n",
    "grad(a) = 2\n",
    "grad(b) = 1\n",
    "'''\n",
    "def dot_product(a_num, b_num):\n",
    "    real = np.dot(a_num.real, b_num.real)\n",
    "    jnum = np.dot(a_num.real, b_num.jnum)\n",
    "    jnum += np.dot(b_num.real, a_num.jnum)\n",
    "    return DualNum(real, jnum)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward result: 11.652071455223084\n",
      "the differential of final result to node1:  5.5\n",
      "the differential of final result to node2:  1.7163378145367738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self,inputs,grads=1):\n",
    "        self.val=inputs\n",
    "        self.grad=grads\n",
    "\n",
    "class Op(object):\n",
    "    def __init__(self,name):\n",
    "        self._name=name\n",
    "        self.node=Node(name)\n",
    "        self.inputs=[]\n",
    "        self.node.grads=0\n",
    "    def compute(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "    def gradient(self, nodes):\n",
    "        assert False, \"Implemented in subclass\"\n",
    "        \n",
    "\n",
    "# 实际计算在各个操作算子上   \n",
    "class Const(Op):\n",
    "    def compute(self, val):\n",
    "        self.node.val=val\n",
    "        self.inputs=[]\n",
    "        return val\n",
    "    def gradient(self,out_grad):\n",
    "        grads=out_grad\n",
    "        self.node.grads=grads\n",
    "        return grads\n",
    "\n",
    "       \n",
    "class MulOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs= node1.val * node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "    \n",
    "    def gradient(self, out_grad):\n",
    "        self.node.grads=out_grad\n",
    "        grads=[self.inputs[1].val * out_grad ,self.inputs[0].val*out_grad]\n",
    "        self.inputs[0].grads+=grads[0]\n",
    "        self.inputs[1].grads+=grads[1]\n",
    "        \n",
    "class AddOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs = node1.val+ node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "        \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad\n",
    "        self.inputs[1].grads+=out_grad\n",
    "        \n",
    "        \n",
    "class SubOp(Op):\n",
    "    def compute(self, node1, node2):\n",
    "        inputs = node1.val-node2.val\n",
    "        self.inputs=[node1,node2]\n",
    "        self.node.val=inputs\n",
    "        \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad\n",
    "        self.inputs[1].grads-=out_grad\n",
    "\n",
    "class In(Op):\n",
    "    def compute(self, node1):\n",
    "        inputs=math.log(node1.val,math.e)\n",
    "        self.inputs=[node1]\n",
    "        self.node.val=inputs\n",
    "    \n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad*(1/self.inputs[0].val)\n",
    "    \n",
    "class Sin(Op):\n",
    "    def compute(self, node):\n",
    "        inputs=np.sin(node.val)\n",
    "        self.inputs=[node]\n",
    "        self.node.val=inputs\n",
    "    def gradient(self, out_grad):\n",
    "        self.inputs[0].grads+=out_grad*np.cos(self.inputs[0].val)\n",
    "    \n",
    "    \n",
    "node1=Const(\"const1\")\n",
    "node1.compute(2)\n",
    "node2=Const(\"const2\")\n",
    "node2.compute(5)\n",
    "\n",
    "in_op=In(\"in\")\n",
    "in_op.compute(node1.node)\n",
    "mul_op=MulOp(\"mul1\")\n",
    "mul_op.compute(node1.node,node2.node)\n",
    "sin_op=Sin(\"sin\")\n",
    "sin_op.compute(node2.node)\n",
    "add_op=AddOp(\"add\")\n",
    "add_op.compute(in_op.node,mul_op.node)\n",
    "sub_op=SubOp(\"sub\")\n",
    "sub_op.compute(add_op.node,sin_op.node)\n",
    "print(\"forward result:\",sub_op.node.val)\n",
    "\n",
    "sub_op.gradient(1)\n",
    "add_op.gradient(sub_op.inputs[0].grads)\n",
    "in_op.gradient(add_op.inputs[0].grads)\n",
    "mul_op.gradient(add_op.inputs[1].grads)\n",
    "sin_op.gradient(sub_op.inputs[1].grads)\n",
    "print(\"the differential of final result to node1: \", node1.node.grads)\n",
    "print(\"the differential of final result to node2: \", node2.node.grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
